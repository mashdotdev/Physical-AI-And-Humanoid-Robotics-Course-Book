# Quickstart: Vision-Language-Action (VLA) Module

This document provides a quick overview of how to get started with the VLA module.

## Prerequisites

- Ubuntu 22.04 LTS
- ROS 2 Humble/Iron
- NVIDIA Isaac Sim
- NVIDIA RTX 4070 Ti or better

## Installation

1.  Clone the repository:
    ```bash
    git clone https://github.com/your-username/hackathon-book.git
    ```
2.  Build the ROS 2 workspace:
    ```bash
    cd hackathon-book
    colcon build
    ```
3.  Source the workspace:
    ```bash
    source install/setup.bash
    ```

## Running the VLA Module

1.  Launch the simulation environment:
    ```bash
    ros2 launch vla_bringup simulation.launch.py
    ```
2.  Run the VLA module:
    ```bash
    ros2 launch vla_bringup vla.launch.py
    ```
3.  Give a voice command, for example:
    > "Pick up the red box"

## Key ROS 2 Topics and Services

-   `/audio_capture/audio`: Audio stream from the microphone.
-   `/speech_to_text/text`: Recognized text from the voice command.
-   `/llm/plan`: The high-level plan generated by the LLM.
-   `/get_plan`: Service to get a plan from the LLM.
-   `/execute_plan`: Action to execute a plan.
