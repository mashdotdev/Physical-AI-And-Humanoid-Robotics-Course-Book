# Research: Vision-Language-Action (VLA) Module

This document outlines the research and decisions made for the VLA module.

## Technology Stack and Integration

The VLA module requires the integration of several a key technologies. This section details the role of each component and the plan for their integration.

### 1. Speech-to-Text: OpenAI Whisper

- **Decision**: Use OpenAI Whisper for converting voice commands to text.
- **Rationale**: Whisper is a state-of-the-art speech recognition system that is robust to noise and can be run locally.
- **Alternatives considered**: Google Speech-to-Text, Mozilla DeepSpeech. Whisper was chosen for its performance and the ability to run it on-premise.
- **Integration**: A ROS 2 node will be created that subscribes to an audio stream from a microphone, sends the audio to a Whisper service, and publishes the resulting text to a topic.

### 2. Natural Language Understanding: OpenAI GPT-4o

- **Decision**: Use OpenAI GPT-4o for parsing natural language commands and generating high-level plans.
- **Rationale**: GPT-4o is a powerful large language model capable of understanding complex instructions and generating structured output.
- **Alternatives considered**: Other LLMs like Llama 3 or Claude 3. GPT-4o is chosen for its advanced reasoning capabilities and its native integration with vision, which will be useful for this project.
- **Integration**: A ROS 2 node will be created that subscribes to the text topic from the Whisper node. It will send the text to the GPT-4o API with a carefully crafted prompt to extract the user's intent and generate a plan in JSON format. The plan will be published to a topic.

### 3. Perception: NVIDIA Isaac ROS

- **Decision**: Use NVIDIA Isaac ROS for perception, including object detection and localization.
- **Rationale**: Isaac ROS is a collection of GPU-accelerated packages for ROS 2 that are optimized for NVIDIA hardware. It provides state-of-the-art performance for perception tasks.
- **Alternatives considered**: Building a perception stack from scratch using OpenCV and other libraries. Isaac ROS is chosen to save development time and leverage the power of the GPU.
- **Integration**: Isaac ROS packages will be used to process data from the robot's sensors (camera, LiDAR). Object detection will be done using a pre-trained YOLO model integrated with Isaac ROS. VSLAM will be used for localization and mapping.

### 4. Navigation: Nav2

- **Decision**: Use Nav2 for robot navigation.
- **Rationale**: Nav2 is the standard navigation stack in ROS 2. It is highly configurable and provides robust path planning and obstacle avoidance capabilities.
- **Alternatives considered**: Writing a custom navigation system. Nav2 is the clear choice for a project of this complexity.
- **Integration**: Nav2 will be configured to work with the robot's kinematics and sensors. The plan generated by the LLM will be translated into navigation goals for Nav2.

### 5. Manipulation: IK Solver and Grasp Planner

- **Decision**: Use a custom IK (Inverse Kinematics) solver and a grasp planner for manipulation.
- **Rationale**: While there are existing manipulation libraries, for this project, a custom solution will provide more flexibility and will be a better learning experience. The IK solver will be based on standard algorithms, and the grasp planner will be a simple script that generates grasp poses for known objects.
- **Alternatives considered**: MoveIt 2. MoveIt 2 is a powerful manipulation framework, but it has a steep learning curve. For the scope of this project, a simpler custom solution is more appropriate.
- **Integration**: The manipulation will be exposed as a ROS 2 action server. The high-level planner will call this action to pick up and place objects.
